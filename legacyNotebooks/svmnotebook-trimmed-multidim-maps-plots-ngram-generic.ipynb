{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pybloom import BloomFilter\n",
    "import inspect\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "import os\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import time\n",
    "%matplotlib notebook\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn import mixture\n",
    "import math\n",
    "\n",
    "def ngram(X, gramsize):\n",
    "    numele = len(X)\n",
    "    minlen = len(X[0])\n",
    "    maxlen = len(X[0])\n",
    "    \n",
    "    for i in X:\n",
    "        templen = len(i)\n",
    "        if(templen>maxlen):\n",
    "            maxlen=templen\n",
    "        if(templen<minlen):\n",
    "            minlen=templen\n",
    "    reqdim = maxlen - gramsize +1\n",
    "    \n",
    "    Y = []\n",
    "    for i in X:\n",
    "        tempy = []\n",
    "        paddedi = i + \"0\"*(maxlen-len(i))\n",
    "        for j in range(0,len(paddedi)-gramsize+1):\n",
    "            tempy.append(paddedi[j:j+gramsize])\n",
    "        if len(tempy) == 0:\n",
    "            tempy = [paddedi]\n",
    "        templeny = len(tempy)\n",
    "        \n",
    "        Y.append(tempy)\n",
    "    return Y\n",
    "\n",
    "def ngramEnforce(X, gramsize, reqdim):\n",
    "    numele = len(X)\n",
    "    minlen = len(X[0])\n",
    "    maxlen = len(X[0])\n",
    "    for i in X:\n",
    "        templen = len(i)\n",
    "        if(templen>maxlen):\n",
    "            maxlen=templen\n",
    "        if(templen<minlen):\n",
    "            minlen=templen\n",
    "    maxlen = reqdim + gramsize -1\n",
    "    Y = []\n",
    "    for i in X:\n",
    "        tempy = []\n",
    "        paddedi = i + \"0\"*(maxlen-len(i))\n",
    "        for j in range(0,len(paddedi)-gramsize+1):\n",
    "            tempy.append(paddedi[j:j+gramsize])\n",
    "        templeny = len(tempy)\n",
    "        \n",
    "        Y.append(tempy)\n",
    "    return np.array(Y)\n",
    "\n",
    "def ngramMaxDimKnown(Xd, gramsize, maxlen):\n",
    "    X = map(str, Xd)\n",
    "    numele = len(X)\n",
    "    reqdim = maxlen - gramsize +1\n",
    "    \n",
    "    Y = []\n",
    "    for i in X:\n",
    "        tempy = []\n",
    "        paddedi = i + \"0\"*(maxlen-len(i))\n",
    "        for j in range(0,len(paddedi)-gramsize+1):\n",
    "            tempy.append(paddedi[j:j+gramsize])\n",
    "        if len(tempy) == 0:\n",
    "            tempy = [paddedi]\n",
    "        templeny = len(tempy)\n",
    "        \n",
    "        Y.append(tempy)\n",
    "    return Y\n",
    "def ngramEnforceMaxKnown(Xd, gramsize, reqdim):\n",
    "    X = map(str, Xd)\n",
    "    numele = len(X)\n",
    "    maxlen = reqdim + gramsize -1\n",
    "    Y = []\n",
    "    for i in X:\n",
    "        tempy = []\n",
    "        paddedi = i + \"0\"*(maxlen-len(i))\n",
    "        for j in range(0,len(paddedi)-gramsize+1):\n",
    "            tempy.append(paddedi[j:j+gramsize])\n",
    "        templeny = len(tempy)\n",
    "        \n",
    "        Y.append(tempy)\n",
    "    return Y\n",
    "\n",
    "def rstripfn(x):\n",
    "    x = x.rstrip('\\n')\n",
    "    \n",
    "    return x.rstrip('\\n')\n",
    "\n",
    "def getDataInString(filename):\n",
    "    with open(filename) as f1:\n",
    "        lst1 = map(rstripfn,f1.readlines())\n",
    "    return lst1\n",
    "\n",
    "def findngram(gramsize, lst1):\n",
    "    lst1g = ngram(lst1, gramsize)\n",
    "\n",
    "    reqdim = len(lst1g[0])\n",
    "    lst1f = []\n",
    "    for i in lst1g:\n",
    "        lst1f.append(map(float, i))\n",
    "\n",
    "    nplst1 = np.array(lst1f)\n",
    "    return (nplst1,reqdim)\n",
    "\n",
    "def preprocess_train(nplst1):\n",
    "    lstf1 = nplst1.astype(float)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    lstS1 = min_max_scaler.fit_transform(lstf1)\n",
    "    return lstS1, min_max_scaler\n",
    "\n",
    "def preprocess_test(nplst1, min_max_scaler):\n",
    "    lstf1 = nplst1.astype(float)\n",
    "    lstS1 = min_max_scaler.transform(lstf1)\n",
    "    return lstS1\n",
    "\n",
    "def trainfn(lstS1, n_compo, maxiter):\n",
    "    clf = mixture.GaussianMixture(n_components=n_compo, covariance_type='full', max_iter=maxiter)\n",
    "    clf.fit(lstS1)\n",
    "    return clf\n",
    "\n",
    "def findmeansigma(clf, lstS1):\n",
    "    meanscore = np.mean(clf.score_samples(lstS1))\n",
    "    meanvar = np.var(clf.score_samples(lstS1))\n",
    "    sigma = math.sqrt(meanvar)\n",
    "    return (meanscore, sigma)\n",
    "\n",
    "def mypred(X, clf, mean, sigma):\n",
    "    scores = clf.score_samples(X)\n",
    "    print(scores)\n",
    "    predictions = []\n",
    "    for i in scores:\n",
    "        print(\"diff is \",abs(i-mean))\n",
    "        if(abs(i-mean)<=max(thres*sigma, 1.0/1e12)):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "    return predictions\n",
    "def mypredSvm(X, clf):\n",
    "    scores = clf.predict(X)\n",
    "    return clf.predict(X)\n",
    "\n",
    "def calcncompo(n):\n",
    "    if(n>10000):\n",
    "        return 100\n",
    "    elif(n<60):\n",
    "        return max(1, n/10)\n",
    "    else:\n",
    "        return 9\n",
    "\n",
    "def areAllNumeric(l):\n",
    "    l1 = map(isCharacterAscii, l)\n",
    "    return all(l1)\n",
    "\n",
    "def isCharacterAscii(str):\n",
    "    try:\n",
    "        float(str)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def train(filename, sstableID):\n",
    "    lst1 = getDataInString(filename)\n",
    "    if(not areAllNumeric(lst1)):\n",
    "        print(\"returning cause key is not numeric\")\n",
    "        return\n",
    "    if(len(lst1) == 1):\n",
    "        print('returning cause num key is 1')\n",
    "        return\n",
    "    nplst1, reqdim = findngram(gramsize, lst1)\n",
    "    lstS1, min_max_scaler = preprocess_train(nplst1)\n",
    "    n_compo = calcncompo(len(lst1))\n",
    "    clf = trainfn(lstS1, n_compo, maxiter)\n",
    "    mean, sigma = findmeansigma(clf, lstS1)\n",
    "    models[str(sstableID)] = (clf, mean, sigma, reqdim, min_max_scaler)\n",
    "    print('training done with sstableid - ', sstableID)\n",
    "    return\n",
    "\n",
    "def trainIndi(nplst1, sstableID,n_compo):\n",
    "#     lst1 = getDataInString(filename)\n",
    "#     if(not areAllNumeric(lst1)):\n",
    "#         print(\"returning cause key is not numeric\")\n",
    "#         return\n",
    "#     if(len(lst1) == 1):\n",
    "#         print('returning cause num key is 1')\n",
    "#         return\n",
    "#     nplst1, reqdim = findngram(gramsize, lst1)\n",
    "#     print(\"prev mean was \", np.mean(nplst1.transpose()))\n",
    "#     print(\"prev var was \", np.var(nplst1.transpose()))\n",
    "#     lstS1, min_max_scaler = preprocess_train(nplst1)\n",
    "    print(\"passed mean was \",np.mean(nplst1.transpose()))\n",
    "    print(\"passed sigma was \",np.var(nplst1.transpose()))\n",
    "#     n_compo = calcncompo(len(lst1))\n",
    "    clf = trainfn(nplst1, n_compo, maxiter)\n",
    "    print(\"found means \")\n",
    "    print(np.sort(clf.means_.transpose()[0]))\n",
    "    print(\"cov matrix \")\n",
    "    print(clf.covariances_)\n",
    "    print(\"weights :\")\n",
    "    print(clf.weights_)\n",
    "    mean, sigma = findmeansigma(clf, nplst1)\n",
    "    print(\"mean is \",str(mean))\n",
    "    print(\"sigma is \", str(sigma))\n",
    "    reqdim = len(nplst1[0])\n",
    "    models[str(sstableID)] = (clf, mean, sigma, reqdim)\n",
    "    print('training done with sstableid - ', sstableID)\n",
    "    return\n",
    "\n",
    "def trainIndiSvm(nplst1, sstableID, nu, gamma, models):\n",
    "    print(\"passed mean was \",np.mean(nplst1, axis=0))\n",
    "    print(\"passed sigma was \",np.var(nplst1, axis=0))\n",
    "    clf = svm.OneClassSVM(nu=nu, kernel=\"rbf\", gamma=gamma)\n",
    "    clf.fit(nplst1)\n",
    "    reqdim = len(nplst1[0])\n",
    "#     print(\"reqdim is \",reqdim)\n",
    "    models[str(sstableID)] = (clf, reqdim)\n",
    "    print('training done with sstableid - ', sstableID)\n",
    "    return models\n",
    "def test(keyPredstr, sstableID):\n",
    "    if(keyPredstr == ''):\n",
    "        print('returning cause string is empty')\n",
    "        return 0\n",
    "    if(not isCharacterAscii(keyPredstr)):\n",
    "        print('returning cause not numric')\n",
    "        return 0\n",
    "    keyPred = [str(keyPredstr)]\n",
    "    if sstableID in models:\n",
    "        # sizeofmodels()\n",
    "        clf, mean, sigma, reqdim, min_max_scaler = models[str(sstableID)]\n",
    "        keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "        keyPredS = preprocess_test(keyPredg, min_max_scaler)\n",
    "        prediction = mypred(keyPredS, clf, mean, sigma)\n",
    "        return prediction[0]\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def testIndi(keyPredg, sstableID):\n",
    "#     if(keyPredstr == ''):\n",
    "#         print('returning cause string is empty')\n",
    "#         return 0\n",
    "#     if(not isCharacterAscii(keyPredstr)):\n",
    "#         print('returning cause not numric')\n",
    "#         return 0\n",
    "#     keyPred = [str(keyPredstr)]\n",
    "    if str(sstableID) in models:\n",
    "        # sizeofmodels()\n",
    "        clf, mean, sigma, reqdim= models[str(sstableID)]\n",
    "#         keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "#         keyPredS = preprocess_test([keyPredg], min_max_scaler)\n",
    "        prediction = mypred([keyPredg], clf, mean, sigma)\n",
    "        return prediction[0]\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "def testIndiSvm(keyPredg, sstableID, models):\n",
    "#     if(keyPredstr == ''):\n",
    "#         print('returning cause string is empty')\n",
    "#         return 0\n",
    "#     if(not isCharacterAscii(keyPredstr)):\n",
    "#         print('returning cause not numric')\n",
    "#         return 0\n",
    "#     keyPred = [str(keyPredstr)]\n",
    "    if str(sstableID) in models:\n",
    "        # sizeofmodels()\n",
    "        clf, reqdim= models[str(sstableID)]\n",
    "#         keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "#         keyPredS = preprocess_test([keyPredg], min_max_scaler)\n",
    "        prediction = mypredSvm([keyPredg], clf)\n",
    "        return prediction[0]\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "\n",
    "def testIndi2(keyPredg, sstableID):\n",
    "#     if(keyPredstr == ''):\n",
    "#         print('returning cause string is empty')\n",
    "#         return 0\n",
    "#     if(not isCharacterAscii(keyPredstr)):\n",
    "#         print('returning cause not numric')\n",
    "#         return 0\n",
    "#     keyPred = [str(keyPredstr)]\n",
    "    if str(sstableID) in models:\n",
    "        # sizeofmodels()\n",
    "        clf, mean, sigma, reqdim= models[str(sstableID)]\n",
    "#         keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "#         keyPredS = preprocess_test([keyPredg], min_max_scaler)\n",
    "        prediction = mypred(keyPredg, clf, mean, sigma)\n",
    "        return prediction\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "def testIndiSvm2(keyPredg, sstableID, models):\n",
    "#     if(keyPredstr == ''):\n",
    "#         print('returning cause string is empty')\n",
    "#         return 0\n",
    "#     if(not isCharacterAscii(keyPredstr)):\n",
    "#         print('returning cause not numric')\n",
    "#         return 0\n",
    "#     keyPred = [str(keyPredstr)]\n",
    "    if str(sstableID) in models:\n",
    "        # sizeofmodels()\n",
    "        clf, reqdim= models[str(sstableID)]\n",
    "#         keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "#         keyPredS = preprocess_test([keyPredg], min_max_scaler)\n",
    "        prediction = mypredSvm(keyPredg, clf)\n",
    "        return prediction\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "def sizeofmodels():\n",
    "    l = []\n",
    "    for sstableID in models:\n",
    "        clf, mean, sigma, reqdim, min_max_scaler = models[str(sstableID)]\n",
    "        print(\"yo\")\n",
    "        print(clf)\n",
    "        p = pickle.dumps(clf)\n",
    "        tempsize = sys.getsizeof(p)\n",
    "        # print(str(tempsize))\n",
    "        # print(sys.getsizeof(p))\n",
    "        print(sstableID, sys.getsizeof(p))\n",
    "def sumsizeofmodels(models):\n",
    "    agg =0\n",
    "    for sstableID in models:\n",
    "        clf, mean, sigma, reqdim= models[str(sstableID)]\n",
    "        print(\"yo\")\n",
    "        print(clf)\n",
    "        p = pickle.dumps(clf)\n",
    "        tempsize = sys.getsizeof(p)\n",
    "        agg += tempsize\n",
    "        # print(str(tempsize))\n",
    "        # print(sys.getsizeof(p))\n",
    "        print(sstableID, sys.getsizeof(p))\n",
    "    return agg\n",
    "def sumsizeofmodelssvm(models):\n",
    "    agg =0\n",
    "    for sstableID in models:\n",
    "        clf, reqdim= models[str(sstableID)]\n",
    "        print(\"yo\")\n",
    "        print(clf)\n",
    "        p = pickle.dumps(clf)\n",
    "        tempsize = sys.getsizeof(p)\n",
    "        agg += tempsize\n",
    "        # print(str(tempsize))\n",
    "        # print(sys.getsizeof(p))\n",
    "        print(sstableID, sys.getsizeof(p))\n",
    "    return agg\n",
    "def sumsizeofmodelssvmReal(models):\n",
    "    agg =0\n",
    "    for sstableID in models:\n",
    "        tempmodel, reqdim= models[str(sstableID)]\n",
    "        tempsize = 8*(tempmodel.support_vectors_.shape[0]*tempmodel.support_vectors_.shape[1])+ 8*(tempmodel.dual_coef_.shape[0]*tempmodel.dual_coef_.shape[1]) + 4*(tempmodel.support_.shape[0])\n",
    "        agg += tempsize\n",
    "        # print(str(tempsize))\n",
    "        # print(sys.getsizeof(p))\n",
    "        print(sstableID, tempsize)\n",
    "    return agg\n",
    "def sumsizeofbloom(bloomfilters):\n",
    "    agg =0\n",
    "    for f in bloomfilters.values():\n",
    "        agg += f.num_bits\n",
    "    agg = agg/8\n",
    "    return agg\n",
    "\n",
    "def readFileByLines(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lst = f.readlines()\n",
    "    f.close()\n",
    "#     print(lst)\n",
    "#     lst = map(str.rstrip, lst)\n",
    "    return lst\n",
    "def parseVdisk(lst):\n",
    "    lst = [s.split(':') for s in lst]\n",
    "#     vid = []\n",
    "#     blc = [] \n",
    "    mat = []\n",
    "    for ele in lst:\n",
    "#         vid.append(int(ele[1]))\n",
    "#         blc.append(int(ele[2], 16))\n",
    "        mat.append([float(int(ele[1]))/1e0,float(int(ele[2], 16))/1e0])\n",
    "#     print(vid, blc)\n",
    "#     return (vid, blc)\n",
    "    return mat\n",
    "def createNgramsPerSStable(thisssTableData, alltemptableData):\n",
    "    mingram = 4\n",
    "    maxgram = 4\n",
    "    initialDimension = alltemptableData.shape[1]\n",
    "    \n",
    "    extrapolatedData = thisssTableData\n",
    "    findimlist = []\n",
    "    for realFeature in range(initialDimension):\n",
    "    #     not taking care of negative features\n",
    "        thisFeatureData = thisssTableData[:,realFeature]\n",
    "        maxlen = len(str(np.max(alltemptableData[:,realFeature])))\n",
    "        dimlist = []\n",
    "        for gramsize in range(mingram, min(maxgram,maxlen)+1):\n",
    "#             print(thisFeatureData[0], gramsize, maxlen)\n",
    "#             print(gramsize)\n",
    "            thisFeatureDataOfGramSize = np.array(ngramMaxDimKnown(thisFeatureData, gramsize, maxlen),dtype=np.float)\n",
    "#             print(thisFeatureDataOfGramSize[0])\n",
    "#             print(thisFeatureDataOfGramSize.shape)\n",
    "            reqdimForLater = thisFeatureDataOfGramSize.shape[1]\n",
    "#             print(reqdimForLater)\n",
    "            dimlist.append(reqdimForLater)\n",
    "            extrapolatedData = np.append(extrapolatedData, thisFeatureDataOfGramSize, axis=1)\n",
    "        findimlist += dimlist\n",
    "    return (extrapolatedData, findimlist)\n",
    "def createNgrams(ssTableData, alltemptableData):\n",
    "    ngramData = {}\n",
    "    dimlists = []\n",
    "    for i in range(len(ssTableData)):\n",
    "        (bigData, dimlist)=createNgramsPerSStable(ssTableData[i], alltemptableData)\n",
    "        ngramData[i] = bigData\n",
    "        dimlists.append(dimlist)\n",
    "    return (ngramData, dimlists)\n",
    "# def createNgramsPerSStableEnforce(thisssTableData, alltemptableData, dimlist):\n",
    "#     mingram = 3\n",
    "#     maxgram = 7\n",
    "#     initialDimension = alltemptableData.shape[1]\n",
    "    \n",
    "#     extrapolatedData = thisssTableData\n",
    "#     findimlist = []\n",
    "#     for realFeature in range(initialDimension):\n",
    "#     #     not taking care of negative features\n",
    "#         thisFeatureData = thisssTableData[:,realFeature]\n",
    "#         maxlen = len(str(np.max(alltemptableData[:,realFeature])))\n",
    "#         dimlist = []\n",
    "#         for gramsize in range(mingram, min(maxgram,maxlen)+1):\n",
    "# #             print(thisFeatureData[0], gramsize, maxlen)\n",
    "# #             print(gramsize)\n",
    "#             thisFeatureDataOfGramSize = np.array(ngramMaxDimKnown(thisFeatureData, gramsize, maxlen),dtype=np.float)\n",
    "# #             print(thisFeatureDataOfGramSize[0])\n",
    "# #             print(thisFeatureDataOfGramSize.shape)\n",
    "#             reqdimForLater = thisFeatureDataOfGramSize.shape[1]\n",
    "# #             print(reqdimForLater)\n",
    "#             dimlist.append(reqdimForLater)\n",
    "#             extrapolatedData = np.append(extrapolatedData, thisFeatureDataOfGramSize, axis=1)\n",
    "#         findimlist += dimlist\n",
    "#     return (extrapolatedData, findimlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    foldername = \"metadata_maps/vdisk/*.db\"\n",
    "    files = sorted(glob.glob(foldername))\n",
    "    ctr=0\n",
    "    alltemptableData = []\n",
    "    for fle in files:\n",
    "        thisline = readFileByLines(fle)\n",
    "    #     if(len(thisline)>1000):\n",
    "    #         continue\n",
    "        mat = parseVdisk(thisline)\n",
    "        ssTableData[ctr] = np.array(mat).reshape(-1,2)\n",
    "        ctr+=1\n",
    "        alltemptableData+=mat\n",
    "        print(ctr,len(thisline))\n",
    "    # print(lines)\n",
    "    alltemptableData = np.array(alltemptableData).reshape(-1,2)\n",
    "    return (ssTableData, alltemptableData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScalers():\n",
    "    scalers = {}\n",
    "    for i in range(0,len(ssTableData)):\n",
    "        thistableData = ssTableData[i]\n",
    "        scaler = preprocessing.StandardScaler().fit(thistableData)\n",
    "        scalers[i] = scaler\n",
    "    return scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "# alltemptableData = generateGaussArrTwoDim(\n",
    "#     numclustersPerSStable*numSstable, gapBetweenClusters, clusterSigma, numelePerCluster )\n",
    "# np.random.shuffle(alltemptableData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(ssTableData)):\n",
    "#     thistableData = alltemptableData[i*numclustersPerSStable:(i+1)*numclustersPerSStable]\n",
    "#     ssTableData[i] = thistableData.reshape(-1,thistableData.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# thistableData = ssTableData[0]\n",
    "# trainIndiSvm(scalers[0].transform(thistableData),0, nu, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def trainFn(nu, gamma):\n",
    "    # train the model on generated data\n",
    "    models = {}\n",
    "    for i in range(0,len(ssTableData)):\n",
    "        starttime = time.time()\n",
    "        thistableData = ssTableData[i]\n",
    "        models = trainIndiSvm(scalers[i].transform(thistableData),i, nu, gamma, models)\n",
    "        endtime = time.time()\n",
    "        print(str(nu),str(gamma),\"time taken \", (endtime-starttime))\n",
    "    return models\n",
    "def trainFnTol(nu, gamma, tol):\n",
    "    # train the model on generated data\n",
    "    models = {}\n",
    "    for i in range(0,len(ssTableData)):\n",
    "        starttime = time.time()\n",
    "        thistableData = ssTableData[i]\n",
    "        models = trainIndiSvmTol(scalers[i].transform(thistableData),i, nu, gamma, models, tol)\n",
    "        endtime = time.time()\n",
    "        print(str(nu),str(gamma),\"time taken \", (endtime-starttime))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pklfile = open(\"svmmodelstandardscaled_0.1_50\", \"rb\")\n",
    "# models = pkl.load(pklfile)\n",
    "# pklfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# put into bloom filter false negatives\n",
    "def putIntoBloomFilters(models):\n",
    "    bloomfilters = {}\n",
    "    errorsTrain = {}\n",
    "    for i in range(0,len(ssTableData)):\n",
    "        thistableData = ssTableData[i]\n",
    "        thistableDataTranspose0 = thistableData.transpose()[0]\n",
    "        if removeClassifier:\n",
    "            newCapacity = max(len(thistableData),1)\n",
    "            print(\"newCapacity \",newCapacity)\n",
    "            f = BloomFilter(capacity=newCapacity, error_rate=errorRate)\n",
    "            for j in range(0,len(thistableData)):\n",
    "                f.add(separator.join(map(str,thistableData[j])))\n",
    "            bloomfilters[i] = f\n",
    "        else:\n",
    "            falseNegativeAns = testIndiSvm2(scalers[i].transform(thistableData), i, models)\n",
    "        #     print(falseNegativeAns)\n",
    "            numFalseNeg = falseNegativeAns.tolist().count(-1)\n",
    "        #     newCapacity = int(capacityBloom*1.0*numFalseNeg/len(thistableData))\n",
    "            newCapacity = max(numFalseNeg,1)\n",
    "            print(\"newCapacity \",newCapacity)\n",
    "            f = BloomFilter(capacity=newCapacity, error_rate=errorRate)\n",
    "            for j in range(0,len(thistableData)):\n",
    "                if(falseNegativeAns[j]==-1 or removeClassifier):\n",
    "        #             print(\"adding \", thistableDataTranspose0[j])\n",
    "                    f.add(separator.join(map(str,thistableData[j])))\n",
    "            bloomfilters[i] = f\n",
    "        errorsTrain[i] = newCapacity\n",
    "    return (bloomfilters,errorsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def performTests(models, bloomfilters):\n",
    "    # create test data\n",
    "    # For now test data is all data\n",
    "    # calculate false positives for test data\n",
    "    truePositive_c=0\n",
    "    truePositive_bf=0\n",
    "    truePositive=0\n",
    "    falsePositive_c=0\n",
    "    falsePositive_bf=0\n",
    "    falsePositive=0\n",
    "    trueNegative=0\n",
    "    for i in range(0, len(ssTableData)):\n",
    "        print(\"on sstable number \", i)\n",
    "        thistableData = ssTableData[i]\n",
    "        thistableDataTranspose0 = thistableData.transpose()[0]\n",
    "        for elemind in range(0,len(thistableData),skipFactor):\n",
    "            elem = thistableData[elemind]\n",
    "            for j in range(0,len(ssTableData)):\n",
    "                if removeClassifier:\n",
    "                    cAnswer = False\n",
    "                else:\n",
    "                    cAnswer = (testIndiSvm(scalers[j].transform([elem])[0],j, models)==1)\n",
    "    #             print(cAnswer)\n",
    "                if not cAnswer:\n",
    "                    bfAnswer = separator.join(map(str,elem)) in bloomfilters[j]\n",
    "                    finAnswer = bfAnswer\n",
    "                else:\n",
    "                    bfAnswer = None\n",
    "                    finAnswer = cAnswer\n",
    "                if finAnswer:\n",
    "                    if(i==j):\n",
    "                        #True answer true positive\n",
    "                        #break because you found answer\n",
    "                        truePositive+=1\n",
    "                        if bfAnswer is None:\n",
    "                            truePositive_c+=1\n",
    "                        else:\n",
    "                            truePositive_bf+=1\n",
    "                        break\n",
    "                    else:\n",
    "                        #False answer false positive\n",
    "                        falsePositive+=1\n",
    "                        if bfAnswer is None:\n",
    "                            falsePositive_c+=1\n",
    "                        else:\n",
    "                            falsePositive_bf+=1\n",
    "                else:\n",
    "                    if(i==j):\n",
    "                        raise ValueEroor(\"not possible\")\n",
    "                    else:\n",
    "                        #True answer true negative\n",
    "                        trueNegative+=1\n",
    "    fprate = falsePositive*1.0/(falsePositive+trueNegative)\n",
    "    return (fprate,\n",
    "            truePositive_c,truePositive_bf,truePositive,\n",
    "            falsePositive_c,falsePositive_bf,falsePositive,\n",
    "            trueNegative)\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistModel(models, nu, gamma):\n",
    "    pickle_out = open(\"svmModels/svmmodelstandardscaled_\"+namePrefix+str(nu)+\"_\"+str(gamma),\"wb\")\n",
    "    pkl.dump(models, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(nu, gamma):\n",
    "    pickle_out = open(\"svmModels/svmmodelstandardscaled_\"+namePrefixOri+str(nu)+\"_\"+str(gamma),\"rb\")\n",
    "    models = pkl.load(pickle_out)\n",
    "    pickle_out.close()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistBloomFilters(bloomfilters, nu, gamma):\n",
    "    pickle_out = open(\"bloomfilters/bloomfilters_\"+namePrefix+str(nu)+\"_\"+str(gamma),\"wb\")\n",
    "    pkl.dump(bloomfilters, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBloomFilters(nu, gamma):\n",
    "    pickle_out = open(\"bloomfilters/bloomfilters_\"+namePrefix+str(nu)+\"_\"+str(gamma),\"rb\")\n",
    "    bloomfilters = pkl.load(pickle_out)\n",
    "    pickle_out.close()\n",
    "    return bloomfilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistObservations(obs, nu, gamma):\n",
    "    pickle_out = open(\"Observations/observations_\"+namePrefix+str(nu)+\"_\"+str(gamma),\"wb\")\n",
    "    pkl.dump(obs, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadObservations(params):\n",
    "    pickle_out = open(\"Observations/observations_\"+namePrefix+modelHelper.paramsToString(params),\"rb\")\n",
    "    obs = pkl.load(pickle_out)\n",
    "    pickle_out.close()\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistTimes(obs):\n",
    "    pickle_out = open(\"times/times_\"+namePrefix,\"wb\")\n",
    "    pkl.dump(obs, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTimes():\n",
    "    pickle_out = open(\"times/times_\"+namePrefix,\"rb\")\n",
    "    obs = pkl.load(pickle_out)\n",
    "    pickle_out.close()\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(nu, gamma):\n",
    "    print(nu,gamma)\n",
    "    starttime = time.time()\n",
    "    models = trainFn(nu, gamma)\n",
    "    persistModel(models, nu, gamma)\n",
    "#     models = loadModel(nu, gamma)\n",
    "    (bloomfilters, errorTrain) = putIntoBloomFilters(models)\n",
    "    persistBloomFilters(bloomfilters, nu, gamma)\n",
    "    (fprate,\n",
    "    truePositive_c,truePositive_bf,truePositive,\n",
    "    falsePositive_c,falsePositive_bf,falsePositive,\n",
    "    trueNegative) = performTests(models, bloomfilters)\n",
    "    observations = (errorTrain, fprate,\n",
    "                    truePositive_c,truePositive_bf,truePositive,\n",
    "                    falsePositive_c,falsePositive_bf,falsePositive,\n",
    "                    trueNegative)\n",
    "    persistObservations(observations, nu, gamma)\n",
    "    endtime = time.time()\n",
    "    return (endtime-starttime, nu, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramsize = -1\n",
    "maxiter = 100\n",
    "# models = {} # storing models based on sstable ids\n",
    "# nu = 0.1\n",
    "# gamma = 1\n",
    "removeClassifier = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thres = -0.1\n",
    "# bloomfilters = {}\n",
    "capacityBloom = 30000\n",
    "errorRate = 0.1\n",
    "skipFactor = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableData = {}\n",
    "separator = \":\"\n",
    "namePrefix = \"isolationforest\"\n",
    "namePrefixOri = namePrefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolationforest loaded\n"
     ]
    }
   ],
   "source": [
    "# typedata = sys.argv[1]\n",
    "typedata = \"vdisk\"\n",
    "if typedata==\"vdisk\":\n",
    "    import vdiskHelper as dataHelper\n",
    "# typemodel = sys.argv[2]\n",
    "typemodel = \"isolationforest\"\n",
    "if typemodel==\"epsilon\":\n",
    "    import epsilonHelper as modelHelper\n",
    "elif typemodel==\"svmsimple\":\n",
    "    import svmsklearncoreHelper as modelHelper\n",
    "elif typemodel==\"libsvmsch\":\n",
    "    import svmlibsvmHelper as modelHelper\n",
    "elif typemodel==\"isolationforest\":\n",
    "    print(\"isolationforest loaded\")\n",
    "    import isolationForestHelper as modelHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 120691)\n",
      "(2, 126781)\n",
      "(3, 125351)\n",
      "(4, 119538)\n",
      "(5, 126719)\n",
      "(6, 25367)\n",
      "(7, 29100)\n",
      "(8, 28898)\n",
      "(9, 30806)\n",
      "(10, 16550)\n",
      "(11, 35023)\n",
      "(12, 5815)\n",
      "(13, 17015)\n",
      "(14, 405)\n",
      "(15, 4266)\n",
      "(16, 1265)\n",
      "(17, 16967)\n",
      "(18, 1963)\n",
      "(19, 880)\n",
      "(20, 397)\n",
      "(21, 1283)\n",
      "(22, 845)\n",
      "(23, 368)\n",
      "(24, 3071)\n",
      "CPU times: user 16.2 s, sys: 845 ms, total: 17 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# (ssTableData, alltemptableData) = getData()\n",
    "# oriSsTableData = ssTableData.copy()\n",
    "# ssTableData, dimlist = createNgrams(ssTableData, alltemptableData)\n",
    "# scalers = getScalers()\n",
    "(oriSsTableData, alltemptableData, ssTableData) = dataHelper.get()\n",
    "scalers = getScalers()\n",
    "# nus = np.logspace(-3,0,6)\n",
    "# gammas = np.logspace(-3,3,6)\n",
    "# nus = np.logspace(-3,0,6)[2:5]\n",
    "# gammas = np.logspace(-1,3,6)[2:]\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# (ssTableData, alltemptableData) = getData()\n",
    "# scalers = getScalers()\n",
    "# nus = np.logspace(-6,0,20)\n",
    "# gammas = np.logspace(-6,3,20)\n",
    "# # nus = np.logspace(-1,0,1)\n",
    "# # gammas = np.logspace(0,3,1)\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# results = Parallel(n_jobs=num_cores)(delayed(ensemble)(i,j) for i,j in tqdm(product(nus, gammas)))\n",
    "# print(results)\n",
    "# # for nu, gamma in product(nus, gammas):\n",
    "# #     ensemble(nu,gamma)\n",
    "    \n",
    "# # models = trainFn(0.1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mshape = (3,4)\n",
    "# mshape = (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nus = np.logspace(-6,0,mshape[0])\n",
    "# gammas = np.logspace(-6,3,mshape[1])\n",
    "# nus = np.logspace(-3,0,1)\n",
    "# gammas = np.logspace(1,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loadTimes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namePrefix = \"6nu6gamma_ex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8843608498780913"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadObservations(modelHelper.sampleparams)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = modelHelper.loadModel(namePrefixOri, modelHelper.sampleparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '1': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0),\n",
       "  16),\n",
       " '10': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '11': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '12': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '13': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '14': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '15': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '16': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '17': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '18': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '19': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '2': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '20': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '21': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '22': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '23': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '3': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '4': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '5': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '6': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '7': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '8': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16),\n",
       " '9': (IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',\n",
       "          max_features=1.0, max_samples='auto', n_estimators=100,\n",
       "          n_jobs=None, random_state=0, verbose=0), 16)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdic = {}\n",
    "fprates = np.zeros(mshape)\n",
    "fprateslist = []\n",
    "falsenegatives_c = []\n",
    "falsenegatives_c_validation = []\n",
    "falsepositive_c = []\n",
    "falsepositive_bf = []\n",
    "bfsizelist = []\n",
    "modelSizeList = []\n",
    "fpdic = {}\n",
    "for i,j in product(nus,gammas):\n",
    "    print(i,j)\n",
    "    try:\n",
    "        obs = loadObservations(i,j)\n",
    "        fprateslist.append(obs[1])\n",
    "        tempsum = 0\n",
    "        for k in obs[0]:\n",
    "            tempsum += obs[0][k]\n",
    "        falsenegatives_c.append(tempsum)\n",
    "        falsepositive_c.append(obs[5])\n",
    "        falsepositive_bf.append(obs[6])\n",
    "        tempsum = 0\n",
    "        for k in obs[9]:\n",
    "            tempsum += obs[9][k]\n",
    "        falsenegatives_c_validation.append(tempsum)\n",
    "        fpdic[(i,j)] = obs[1]\n",
    "        print(obs[1])\n",
    "    except IOError:\n",
    "        obs = None\n",
    "        fprateslist.append(1)\n",
    "        fpdic[(i,j)] = None\n",
    "        print(obs)\n",
    "    try:\n",
    "        bloomfilters = loadBloomFilters(i,j)\n",
    "        tempsize = sumsizeofbloom(bloomfilters)\n",
    "        bfsizelist.append(tempsize)\n",
    "        print(tempsize)\n",
    "    except IOError:\n",
    "        tempsize = None\n",
    "        bfsizelist.append(1e6)\n",
    "        print(tempsize)\n",
    "    try:\n",
    "        models = loadModel(i,j)\n",
    "        tempsize = sumsizeofmodelssvmReal(models)\n",
    "        modelSizeList.append(tempsize)\n",
    "        print(tempsize)\n",
    "    except IOError:\n",
    "        tempsize = None\n",
    "        modelSizeList.append(1e10)\n",
    "        print(tempsize)\n",
    "    \n",
    "#     fprateslist.append(obs[1])\n",
    "#     fpdic[(i,j)] = obs[1]\n",
    "    obsdic[(i,j)] = obs\n",
    "fprateslist = np.array(fprateslist).reshape(mshape)\n",
    "falsenegatives_c = np.array(falsenegatives_c).reshape(mshape)\n",
    "falsepositive_c = np.array(falsepositive_c).reshape(mshape)\n",
    "falsepositive_bf = np.array(falsepositive_bf).reshape(mshape)\n",
    "falsenegatives_c_validation = np.array(falsenegatives_c_validation).reshape(mshape)\n",
    "bfsizelist = np.array(bfsizelist).reshape(mshape)\n",
    "modelSizeList = np.array(modelSizeList).reshape(mshape)\n",
    "totalSizeList = bfsizelist + modelSizeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in obsdic:\n",
    "    print(obsdic[i][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minval = 1.0\n",
    "for i in obsdic:\n",
    "    print(i, obsdic[i][1])\n",
    "    if obsdic[i][1] < minval:\n",
    "        minval = obsdic[i][1]\n",
    "        minpair =i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsdic[(nus[3],gammas[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadTimes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minpair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSizeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(fprateslist, interpolation='nearest', cmap=plt.cm.hot,\n",
    "           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.yticks(np.arange(len(nus)), nus, rotation=45)\n",
    "plt.xticks(np.arange(len(gammas)), gammas)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(bfsizelist, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(modelSizeList, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(fprateslist, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0,vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(falsenegatives_c, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(falsepositive_c, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(falsepositive_bf, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(falsenegatives_c_validation, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(totalSizeList, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fprateslist * bfsizelist).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestval = (fprateslist[0][0],totalSizeList[0][0])\n",
    "bestpair = (0,0)\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        thisfpr = fprateslist[i][j]\n",
    "        thismsize = totalSizeList[i][j]\n",
    "        if(thisfpr<0.2 and thismsize<bestval[1]):\n",
    "            bestval = (thisfpr,thismsize)\n",
    "            bestpair = (i,j)\n",
    "print(bestpair)\n",
    "print(bestval) \n",
    "print(sumsizeofbloom(coreBf))\n",
    "print(bestval[1]*1.0/sumsizeofbloom(coreBf)*1.0)\n",
    "print(bestval[0]*1.0/coreobs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestval = fprateslist[0][0]\n",
    "bestpair = (0,0)\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        thisfpr = fprateslist[i][j]\n",
    "        thismsize = totalSizeList[i][j]\n",
    "        if(thisfpr<coreobs[1]):\n",
    "            bestval = thisfpr\n",
    "            bestpair = (i,j)\n",
    "print(bestpair)\n",
    "print(bestval) \n",
    "print(sumsizeofbloom(coreBf))\n",
    "print(bestval*1.0/sumsizeofbloom(coreBf)*1.0)\n",
    "print(bestval*1.0/coreobs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreobs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap((np.power(fprateslist,3) * totalSizeList), xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(modelSizeList, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.heatmap(modelSizeList+bfsizelist, xticklabels=gammas, yticklabels = nus, cmap=\"YlGnBu\", vmin=0,vmax = 1e6, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSizeList + bfsizelist*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namePrefix = \"core\"\n",
    "coreobs = loadObservations(0.1,1.0)\n",
    "coreBf = loadBloomFilters(0.1,1.0)\n",
    "print(sumsizeofbloom(coreBf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunumber = 16\n",
    "gammanumber = 18\n",
    "tempmodel = loadModel(nus[nunumber-1],gammas[gammanumber-1])\n",
    "obs = loadObservations(nus[nunumber-1],gammas[gammanumber-1])\n",
    "print(nus[nunumber-1],gammas[gammanumber-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namePrefix = '20nu20gamma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.fit_status_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.offset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tempmodel.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tempmodel.dual_coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tempmodel.dual_coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.dual_coef_[0].tolist().count(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tempmodel.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.support_.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.support_vectors_.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.dual_coef_.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*(tempmodel.support_vectors_.shape[0]*tempmodel.support_vectors_.shape[1])+ 8*(tempmodel.dual_coef_.shape[0]*tempmodel.dual_coef_.shape[1]) + 4*(tempmodel.support_.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempmodel.support_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsizeofmodelssvmReal(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunumber = 18\n",
    "# gammanumber = 18\n",
    "# # tempmodel = loadModel(nus[nunumber-1],gammas[gammanumber-1])\n",
    "# tempmodel = trainFnTol(nus[nunumber-1],gammas[gammanumber-1],1e-5)\n",
    "# print(nus[nunumber-1],gammas[gammanumber-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namePrefix = namePrefix + \"tol1e-5\"\n",
    "# persistModel(tempmodel,nus[nunumber-1],gammas[gammanumber-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableNumber = 2\n",
    "temptableData = ssTableData[ssTableNumber]\n",
    "clf = tempmodel[str(ssTableNumber)][0]\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(\n",
    "        min(scalers[ssTableNumber].transform(temptableData.reshape(-1,2)).T[0]), \n",
    "        max(scalers[ssTableNumber].transform(temptableData.reshape(-1,2)).T[0]), 500), \n",
    "    np.linspace(\n",
    "        min(scalers[ssTableNumber].transform(temptableData.reshape(-1,2)).T[1]), \n",
    "        max(scalers[ssTableNumber].transform(temptableData.reshape(-1,2)).T[1]), 500))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(),yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "thisScaleddata = scalers[ssTableNumber].transform(temptableData)\n",
    "thistablepredicitons = clf.predict(thisScaleddata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = []\n",
    "for i in range(len(ssTableData)):\n",
    "    if i==ssTableNumber:\n",
    "        continue\n",
    "    thistabledata = ssTableData[i]\n",
    "    for j in range(0, len(thistabledata), skipFactor):\n",
    "        testData.append(thistabledata[j])\n",
    "testData = np.array(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testScaleddata = scalers[ssTableNumber].transform(testData)\n",
    "testtablepredicitons = clf.predict(testScaleddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorTestCases = np.array([testScaleddata[i] for i in range(len(testScaleddata)) if testtablepredicitons[i]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorTrainCases = np.array([thisScaleddata[i] for i in range(len(thisScaleddata)) if thistablepredicitons[i]==-1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "s = 40\n",
    "\n",
    "# testData = np.array([ssTableData[i] for i in ssTableData if i!=ssTableNumber]).reshape(-1,2)\n",
    "b2 = plt.scatter(errorTestCases[:, 0], errorTestCases[:, 1], c='blueviolet', s=s, edgecolors='k')\n",
    "# b1 = plt.scatter(\n",
    "#     scalers[ssTableNumber].transform(ssTableData[ssTableNumber])[:, 0], \n",
    "#     scalers[ssTableNumber].transform(ssTableData[ssTableNumber])[:, 1], c='white', s=s, edgecolors='k')\n",
    "# b3 = plt.scatter(errorTrainCases[:,0], errorTrainCases[:,1], c='yellow', s=s, edgecolors='k')\n",
    "# b4 = plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], c= 'green', s=s, edgecolors='k')\n",
    "# c = plt.scatter(X_outliers[:, 0], X_outliers[:, 0]*0+2, c='gold', s=s,\n",
    "#                 edgecolors='k')\n",
    "plt.axis('tight')\n",
    "# plt.xlim((-5, 5))\n",
    "# plt.ylim((-5, 5))\n",
    "# plt.show()\n",
    "plt.legend([a.collections[0], b2],\n",
    "           [\"learned frontier\", \"error test observations\"],\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"error train: %d/%d ; errors test: %d/%d ; false positives: %d ; fpr: %f\"\n",
    "    % (len(errorTrainCases), len(thisScaleddata), len(errorTestCases), len(testScaleddata), obs[7], obs[1]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"plots/\"+namePrefix + str(nus[nunumber-1])+\"_\"+str(gammas[gammanumber-1]) +\"_\"+ str(ssTableNumber) +\"_\"+ str(nunumber)+\"_\"+str(gammanumber)+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "errorTrainCases =  np.array(errorTrainCases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(scalers[0].transform, ssTableData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisScaleddata = scalers[ssTableNumber].transform(temptableData)\n",
    "thistablepredicitons = clf.predict(thisScaleddata)\n",
    "[thisScaleddata[i] for i in range(len(thisScaleddata)) if thistablepredicitons[i]==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thistablepredicitons.tolist().count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thistablepredicitons.tolist().count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(Z.min(), 0, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempscores = clf.score_samples(thisScaleddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongscores = [tempscores[i] for i in range(len(thisScaleddata)) if thistablepredicitons[i]==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(wrongscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(wrongscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempscores = tempscores.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempscores.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(wrongscores + clf.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sort(wrongscores + clf.intercept_[0]), normed=True, bins=40)\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(tempscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorTrainCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoressup = clf.score_samples(clf.support_vectors_)\n",
    "predictionssup = clf.predict(clf.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(scoressup+clf.intercept_[0])[-15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sort(scoressup+clf.intercept_[0]), normed=True, bins=400)\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scoressup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
