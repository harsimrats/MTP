{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "class ListOfStrings:\n",
    "    def __init__(self, listOfStrings):\n",
    "        self.listOfStrings = listOfStrings\n",
    "        self.maxlen = np.max(list(map(len, listOfStrings)))\n",
    "def gramize(listOfStrings, gramsize):\n",
    "    X = listOfStrings.listOfStrings\n",
    "    numele = len(X)\n",
    "    reqdim = listOfStrings.maxlen - gramsize +1 \n",
    "    Y = []\n",
    "    for i in X:\n",
    "        tempy = []\n",
    "        paddedi = i + \"0\"*(listOfStrings.maxlen-len(i))\n",
    "        for j in range(0,len(paddedi)-gramsize+1):\n",
    "            tempy.append(paddedi[j:j+gramsize])\n",
    "        if len(tempy) == 0:\n",
    "            tempy = [paddedi]\n",
    "        templeny = len(tempy)\n",
    "        Y.append(tempy)\n",
    "    return np.array(Y)\n",
    "def ngramize(mingram, maxgram, listOfStrings):\n",
    "    ngramData = np.array([])\n",
    "    print(\"maxlen is\")\n",
    "    print(listOfStrings.maxlen)\n",
    "    for gramsize in range(mingram, min(maxgram,listOfStrings.maxlen)+1):\n",
    "        gramData = gramize(listOfStrings, gramsize)\n",
    "        if ngramData.shape[0] == 0:\n",
    "            ngramData = gramData.copy()\n",
    "        else:\n",
    "            ngramData = np.append(ngramData, gramData, axis=1)\n",
    "    return ngramData\n",
    "def removeHash(listOfStrings):\n",
    "    return np.array([\":\".join(x.split(\":\")[1:]) for x in listOfStrings])\n",
    "def getSentences(listOfStrings,mingram, maxgram, maxlen=None):\n",
    "    listOfStringstemp = removeHash(listOfStrings)\n",
    "    print(listOfStringstemp)\n",
    "    listOfStringstemp = ListOfStrings(listOfStringstemp)\n",
    "    if maxlen is not None:\n",
    "        listOfStringstemp.maxlen = maxlen\n",
    "    listOfListOfStringstemp = ngramize(mingram, maxgram, listOfStringstemp)\n",
    "    print(listOfListOfStringstemp)\n",
    "    listOfSentences = np.array([\" \".join(x) for x in listOfListOfStringstemp])\n",
    "    return listOfSentences, listOfStringstemp.maxlen\n",
    "class BagOfWordsv1:\n",
    "    def __init__(self, listOfStrings, mingram, maxgram):\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.mingram = mingram\n",
    "        self.maxgram = maxgram\n",
    "        listOfSentences, self.maxlen = getSentences(listOfStrings, mingram, maxgram)\n",
    "        baggedData = self.vectorizer.fit_transform(listOfSentences)\n",
    "        self.tfidf = TfidfTransformer().fit(baggedData)\n",
    "    def process(self, listOfStrings):\n",
    "        listOfSentences, _ = getSentences(listOfStrings,self.mingram, self.maxgram, maxlen=self.maxlen)\n",
    "        baggedData = self.vectorizer.transform(listOfSentences)\n",
    "        baggedData = self.tfidf.transform(baggedData)\n",
    "        return baggedData\n",
    "def readFileByLines(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lst = f.readlines()\n",
    "    f.close()\n",
    "#     print(lst)\n",
    "    lst = list(map(str.rstrip, lst))\n",
    "    return lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")\n",
    "foldername = \"metadata_maps/vdisk/*.db\"\n",
    "files = sorted(glob.glob(foldername))\n",
    "ctr=0\n",
    "alltemptableDatastr = []\n",
    "ssTableDatastr = {}\n",
    "for fle in files:\n",
    "    thisline = readFileByLines(fle)\n",
    "    # if(len(thisline)>1000):\n",
    "    #     continue\n",
    "    # thisline = [x.strip('\\n') for x in thisline]\n",
    "\n",
    "    ssTableDatastr[ctr] = np.array(thisline)\n",
    "    ctr+=1\n",
    "    alltemptableDatastr += thisline\n",
    "    print(ctr,len(thisline))\n",
    "# print(lines)\n",
    "alltemptableDatastr = np.array(alltemptableDatastr)\n",
    "print(ssTableDatastr)\n",
    "print(alltemptableDatastr)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWords = BagOfWordsv1(alltemptableDatastr,6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableDatastr0sparse = bagOfWords.process(ssTableDatastr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableDatastr0sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableDatastr0sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableDatastr0sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ssTableDatastr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_train.data)\n",
    "\n",
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf[0].todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts[0].todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf[0].todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
