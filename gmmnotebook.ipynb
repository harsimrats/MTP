{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pybloom import BloomFilter\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGauss(numcompo, gap, sigma, numelepercompo):\n",
    "    mu = 0\n",
    "    ans = []\n",
    "    for i in range(0, numcompo):\n",
    "        s = np.random.normal(mu, sigma, numelepercompo)\n",
    "#         print(s)\n",
    "        ans += s.tolist()\n",
    "        mu += gap\n",
    "    return np.array([ans]).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGaussArr(numcompo, gap, sigma, numelepercompo):\n",
    "    mu = 0\n",
    "    ans = []\n",
    "    for i in range(0, numcompo):\n",
    "        s = np.random.normal(mu, sigma, numelepercompo)\n",
    "#         print(s)\n",
    "        ans.append(s)\n",
    "        mu += gap\n",
    "    return np.array(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generateGauss(2, 0.2, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sArr = generateGaussArr(2, 0.2, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(sArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sArr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sArr[0:2].flatten().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, bins, ignored = plt.hist(s, 30, normed=True)\n",
    "# plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 100]]  # diagonal covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.random.multivariate_normal(mean, cov, 5000).T\n",
    "plt.plot(x, y, 'x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = np.identity(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = diag*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.random.multivariate_normal(mu,diag,100000).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([x,y,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "l.append(x).append(y).append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=l.appe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat = generateGauss(100, 0.4, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(datat.transpose()[0],500,normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIndi(datat,\"1\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = models['1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempans = []\n",
    "for i in range(0,len(clf.means_)):\n",
    "    thiscov = math.sqrt(clf.covariances_[i][0][0])\n",
    "#     print(thiscov)\n",
    "    thisweight = int(clf.weights_[i]*100*100)\n",
    "#     print(thisweight)\n",
    "    temps = np.random.normal(clf.means_[i],thiscov, thisweight)\n",
    "    tempans += temps.tolist()\n",
    "tempans = np.array([tempans]).transpose()\n",
    "plt.hist(tempans.transpose()[0], 500, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in datat.transpose()[0]:\n",
    "    tempans = testIndi([i],'1')\n",
    "    if(tempans==1):\n",
    "        count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)\n",
    "print(len(datat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from sklearn import mixture\n",
    "import math\n",
    "\n",
    "def ngram(X, gramsize):\n",
    "    numele = len(X)\n",
    "    minlen = len(X[0])\n",
    "    maxlen = len(X[0])\n",
    "    \n",
    "    for i in X:\n",
    "        templen = len(i)\n",
    "        if(templen>maxlen):\n",
    "            maxlen=templen\n",
    "        if(templen<minlen):\n",
    "            minlen=templen\n",
    "    reqdim = maxlen - gramsize +1\n",
    "    \n",
    "    Y = []\n",
    "    for i in X:\n",
    "        tempy = []\n",
    "        paddedi = i + \"0\"*(maxlen-len(i))\n",
    "        for j in range(0,len(paddedi)-gramsize+1):\n",
    "            tempy.append(paddedi[j:j+gramsize])\n",
    "        if len(tempy) == 0:\n",
    "            tempy = [paddedi]\n",
    "        templeny = len(tempy)\n",
    "        \n",
    "        Y.append(tempy)\n",
    "    return Y\n",
    "\n",
    "def ngramEnforce(X, gramsize, reqdim):\n",
    "    numele = len(X)\n",
    "    minlen = len(X[0])\n",
    "    maxlen = len(X[0])\n",
    "    for i in X:\n",
    "        templen = len(i)\n",
    "        if(templen>maxlen):\n",
    "            maxlen=templen\n",
    "        if(templen<minlen):\n",
    "            minlen=templen\n",
    "    maxlen = reqdim + gramsize -1\n",
    "    Y = []\n",
    "    for i in X:\n",
    "        tempy = []\n",
    "        paddedi = i + \"0\"*(maxlen-len(i))\n",
    "        for j in range(0,len(paddedi)-gramsize+1):\n",
    "            tempy.append(paddedi[j:j+gramsize])\n",
    "        templeny = len(tempy)\n",
    "        \n",
    "        Y.append(tempy)\n",
    "    return np.array(Y)\n",
    "\n",
    "def rstripfn(x):\n",
    "    x = x.rstrip('\\n')\n",
    "    \n",
    "    return x.rstrip('\\n')\n",
    "\n",
    "def getDataInString(filename):\n",
    "    with open(filename) as f1:\n",
    "        lst1 = map(rstripfn,f1.readlines())\n",
    "    return lst1\n",
    "\n",
    "def findngram(gramsize, lst1):\n",
    "    lst1g = ngram(lst1, gramsize)\n",
    "\n",
    "    reqdim = len(lst1g[0])\n",
    "    lst1f = []\n",
    "    for i in lst1g:\n",
    "        lst1f.append(map(float, i))\n",
    "\n",
    "    nplst1 = np.array(lst1f)\n",
    "    return (nplst1,reqdim)\n",
    "\n",
    "def preprocess_train(nplst1):\n",
    "    lstf1 = nplst1.astype(float)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    lstS1 = min_max_scaler.fit_transform(lstf1)\n",
    "    return lstS1, min_max_scaler\n",
    "\n",
    "def preprocess_test(nplst1, min_max_scaler):\n",
    "    lstf1 = nplst1.astype(float)\n",
    "    lstS1 = min_max_scaler.transform(lstf1)\n",
    "    return lstS1\n",
    "\n",
    "def trainfn(lstS1, n_compo, maxiter):\n",
    "    clf = mixture.GaussianMixture(n_components=n_compo, covariance_type='full', max_iter=maxiter)\n",
    "    clf.fit(lstS1)\n",
    "    return clf\n",
    "\n",
    "def findmeansigma(clf, lstS1):\n",
    "    meanscore = np.mean(clf.score_samples(lstS1))\n",
    "    meanvar = np.var(clf.score_samples(lstS1))\n",
    "    sigma = math.sqrt(meanvar)\n",
    "    return (meanscore, sigma)\n",
    "\n",
    "def mypred(X, clf, mean, sigma):\n",
    "    scores = clf.score_samples(X)\n",
    "    predictions = []\n",
    "    for i in scores:\n",
    "        if(abs(i-mean)<thres*sigma):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "    return predictions\n",
    "\n",
    "def calcncompo(n):\n",
    "    if(n>10000):\n",
    "        return 100\n",
    "    elif(n<60):\n",
    "        return max(1, n/10)\n",
    "    else:\n",
    "        return 9\n",
    "\n",
    "def areAllNumeric(l):\n",
    "    l1 = map(isCharacterAscii, l)\n",
    "    return all(l1)\n",
    "\n",
    "def isCharacterAscii(str):\n",
    "    try:\n",
    "        float(str)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def train(filename, sstableID):\n",
    "    lst1 = getDataInString(filename)\n",
    "    if(not areAllNumeric(lst1)):\n",
    "        print(\"returning cause key is not numeric\")\n",
    "        return\n",
    "    if(len(lst1) == 1):\n",
    "        print('returning cause num key is 1')\n",
    "        return\n",
    "    nplst1, reqdim = findngram(gramsize, lst1)\n",
    "    lstS1, min_max_scaler = preprocess_train(nplst1)\n",
    "    n_compo = calcncompo(len(lst1))\n",
    "    clf = trainfn(lstS1, n_compo, maxiter)\n",
    "    mean, sigma = findmeansigma(clf, lstS1)\n",
    "    models[str(sstableID)] = (clf, mean, sigma, reqdim, min_max_scaler)\n",
    "    print('training done with sstableid - ', sstableID)\n",
    "    return\n",
    "\n",
    "def trainIndi(nplst1, sstableID,n_compo):\n",
    "#     lst1 = getDataInString(filename)\n",
    "#     if(not areAllNumeric(lst1)):\n",
    "#         print(\"returning cause key is not numeric\")\n",
    "#         return\n",
    "#     if(len(lst1) == 1):\n",
    "#         print('returning cause num key is 1')\n",
    "#         return\n",
    "#     nplst1, reqdim = findngram(gramsize, lst1)\n",
    "#     print(\"prev mean was \", np.mean(nplst1.transpose()))\n",
    "#     print(\"prev var was \", np.var(nplst1.transpose()))\n",
    "#     lstS1, min_max_scaler = preprocess_train(nplst1)\n",
    "    print(\"passed mean was \",np.mean(nplst1.transpose()))\n",
    "    print(\"passed sigma was \",np.var(nplst1.transpose()))\n",
    "#     n_compo = calcncompo(len(lst1))\n",
    "    clf = trainfn(nplst1, n_compo, maxiter)\n",
    "    print(\"found means \")\n",
    "    print(np.sort(clf.means_.transpose()[0]))\n",
    "    print(\"cov matrix \")\n",
    "    print(clf.covariances_)\n",
    "    print(\"weights :\")\n",
    "    print(clf.weights_)\n",
    "    mean, sigma = findmeansigma(clf, nplst1)\n",
    "    print(\"mean is \",str(mean))\n",
    "    print(\"sigma is \", str(sigma))\n",
    "    reqdim = len(nplst1[0])\n",
    "    models[str(sstableID)] = (clf, mean, sigma, reqdim)\n",
    "    print('training done with sstableid - ', sstableID)\n",
    "    return\n",
    "\n",
    "def test(keyPredstr, sstableID):\n",
    "    if(keyPredstr == ''):\n",
    "        print('returning cause string is empty')\n",
    "        return 0\n",
    "    if(not isCharacterAscii(keyPredstr)):\n",
    "        print('returning cause not numric')\n",
    "        return 0\n",
    "    keyPred = [str(keyPredstr)]\n",
    "    if sstableID in models:\n",
    "        # sizeofmodels()\n",
    "        clf, mean, sigma, reqdim, min_max_scaler = models[str(sstableID)]\n",
    "        keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "        keyPredS = preprocess_test(keyPredg, min_max_scaler)\n",
    "        prediction = mypred(keyPredS, clf, mean, sigma)\n",
    "        return prediction[0]\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def testIndi(keyPredg, sstableID):\n",
    "#     if(keyPredstr == ''):\n",
    "#         print('returning cause string is empty')\n",
    "#         return 0\n",
    "#     if(not isCharacterAscii(keyPredstr)):\n",
    "#         print('returning cause not numric')\n",
    "#         return 0\n",
    "#     keyPred = [str(keyPredstr)]\n",
    "    if str(sstableID) in models:\n",
    "        # sizeofmodels()\n",
    "        clf, mean, sigma, reqdim= models[str(sstableID)]\n",
    "#         keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "#         keyPredS = preprocess_test([keyPredg], min_max_scaler)\n",
    "        prediction = mypred([keyPredg], clf, mean, sigma)\n",
    "        return prediction[0]\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "\n",
    "def testIndi2(keyPredg, sstableID):\n",
    "#     if(keyPredstr == ''):\n",
    "#         print('returning cause string is empty')\n",
    "#         return 0\n",
    "#     if(not isCharacterAscii(keyPredstr)):\n",
    "#         print('returning cause not numric')\n",
    "#         return 0\n",
    "#     keyPred = [str(keyPredstr)]\n",
    "    if str(sstableID) in models:\n",
    "        # sizeofmodels()\n",
    "        clf, mean, sigma, reqdim= models[str(sstableID)]\n",
    "#         keyPredg = ngramEnforce(keyPred, gramsize, reqdim)\n",
    "#         keyPredS = preprocess_test([keyPredg], min_max_scaler)\n",
    "        prediction = mypred(keyPredg, clf, mean, sigma)\n",
    "        return prediction\n",
    "    else:\n",
    "        print(str(sstableID), ' - sstableid not in models')\n",
    "        return 0\n",
    "    \n",
    "def sizeofmodels():\n",
    "    l = []\n",
    "    for sstableID in models:\n",
    "        clf, mean, sigma, reqdim, min_max_scaler = models[str(sstableID)]\n",
    "        print(\"yo\")\n",
    "        print(clf)\n",
    "        p = pickle.dumps(clf)\n",
    "        tempsize = sys.getsizeof(p)\n",
    "        # print(str(tempsize))\n",
    "        # print(sys.getsizeof(p))\n",
    "        print(sstableID, sys.getsizeof(p))\n",
    "def sumsizeofmodels(models):\n",
    "    agg =0\n",
    "    for sstableID in models:\n",
    "        clf, mean, sigma, reqdim= models[str(sstableID)]\n",
    "        print(\"yo\")\n",
    "        print(clf)\n",
    "        p = pickle.dumps(clf)\n",
    "        tempsize = sys.getsizeof(p)\n",
    "        agg += tempsize\n",
    "        # print(str(tempsize))\n",
    "        # print(sys.getsizeof(p))\n",
    "        print(sstableID, sys.getsizeof(p))\n",
    "    return agg\n",
    "def sumsizeofbloom(bloomfilters):\n",
    "    agg =0\n",
    "    for f in bloomfilters.values():\n",
    "        agg += f.num_bits\n",
    "    agg = agg/8\n",
    "    return agg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramsize = -1\n",
    "maxiter = 100\n",
    "thres = 1\n",
    "models = {} # storing models based on sstable ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = BloomFilter(capacity=30, error_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f.add(0.01*x, skip_check=True) for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all([(0.01*x in f) for x in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10 in f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.num_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "\n",
    "for i in range(0,numSstable):\n",
    "    thistableData = generateGauss(numclustersPerSStable, gapBetweenClusters, clusterSigma, numelePerCluster )\n",
    "    ssTableData[i] = thistableData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numclustersPerSStable = 100\n",
    "numclustersPerSStableForTrain = numclustersPerSStable\n",
    "numSstable = 30\n",
    "numelePerCluster = 300\n",
    "gapBetweenClusters = 0.5\n",
    "clusterSigma = 0.1\n",
    "ssTableData = {}\n",
    "models = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 0.1\n",
    "bloomfilters = {}\n",
    "capacityBloom = 30000\n",
    "errorRate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "alltemptableData = generateGaussArr(\n",
    "    numclustersPerSStable*numSstable, gapBetweenClusters, clusterSigma, numelePerCluster )\n",
    "np.random.shuffle(alltemptableData)\n",
    "for i in range(0,numSstable):\n",
    "    thistableData = alltemptableData[i*numclustersPerSStable:(i+1)*numclustersPerSStable].flatten()\n",
    "    ssTableData[i] = thistableData.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssTableData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alltemptableData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model on generated data\n",
    "for i in range(0,numSstable):\n",
    "    thistableData = ssTableData[i]\n",
    "    trainIndi(thistableData,i, numclustersPerSStableForTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# put into bloom filter false negatives\n",
    "for i in range(0,numSstable):\n",
    "    thistableData = ssTableData[i]\n",
    "    thistableDataTranspose0 = thistableData.transpose()[0]\n",
    "    falseNegativeAns = testIndi2(thistableData, i)\n",
    "    numFalseNeg = falseNegativeAns.count(-1)\n",
    "#     newCapacity = int(capacityBloom*1.0*numFalseNeg/len(thistableData))\n",
    "    newCapacity = max(numFalseNeg,1)\n",
    "    print(\"newCapacity \",newCapacity)\n",
    "    f = BloomFilter(capacity=newCapacity, error_rate=errorRate)\n",
    "    for j in range(0,len(thistableDataTranspose0)):\n",
    "        if(falseNegativeAns[j]==-1):\n",
    "#             print(\"adding \", thistableDataTranspose0[j])\n",
    "            f.add(thistableDataTranspose0[j])\n",
    "    bloomfilters[i] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create test data\n",
    "# For now test data is all data\n",
    "# calculate false positives for test data\n",
    "truePositive=0\n",
    "falsePositive=0\n",
    "trueNegative=0\n",
    "for i in range(0, numSstable):\n",
    "    print(\"on sstable number \", i)\n",
    "    thistableData = ssTableData[i]\n",
    "    thistableDataTranspose0 = thistableData.transpose()[0]\n",
    "    for elemind in range(0,len(thistableDataTranspose0),50):\n",
    "        elem = thistableDataTranspose0[elemind]\n",
    "        for j in range(0,numSstable):\n",
    "            finAnswer = (testIndi([elem],j)==1)\n",
    "            if not finAnswer:\n",
    "                finAnswer = elem in bloomfilters[j]\n",
    "            if finAnswer:\n",
    "                if(i==j):\n",
    "                    #True answer true positive\n",
    "                    #break because you found answer\n",
    "                    truePositive+=1\n",
    "                    break\n",
    "                else:\n",
    "                    #False answer false positive\n",
    "                    falsePositive+=1\n",
    "            else:\n",
    "                if(i==j):\n",
    "                    raise ValueEroor(\"not possible\")\n",
    "                else:\n",
    "                    #True answer true negative\n",
    "                    trueNegative+=1\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(truePositive,falsePositive,trueNegative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprate = falsePositive*1.0/(falsePositive+trueNegative)\n",
    "print(fprate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsizeofmodels(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumsizeofbloom(bloomfilters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(models['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    else:\n",
    "        print(type(obj))\n",
    "        print(\"cases left\")\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from types import ModuleType, FunctionType\n",
    "from gc import get_referents\n",
    "\n",
    "# Custom objects know their class.\n",
    "# Function objects seem to know way too much, including modules.\n",
    "# Exclude modules as well.\n",
    "BLACKLIST = type, ModuleType, FunctionType\n",
    "\n",
    "\n",
    "def getsize(obj):\n",
    "    \"\"\"sum size of object & members.\"\"\"\n",
    "    if isinstance(obj, BLACKLIST):\n",
    "        raise TypeError('getsize() does not take argument of type: '+ str(type(obj)))\n",
    "    seen_ids = set()\n",
    "    size = 0\n",
    "    objects = [obj]\n",
    "    while objects:\n",
    "        need_referents = []\n",
    "        for obj in objects:\n",
    "            if not isinstance(obj, BLACKLIST) and id(obj) not in seen_ids:\n",
    "                seen_ids.add(id(obj))\n",
    "                print(type(obj))\n",
    "                print(sys.getsizeof(obj))\n",
    "                size += sys.getsizeof(obj)\n",
    "                print(size)\n",
    "                need_referents.append(obj)\n",
    "        objects = get_referents(*need_referents)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsize(models['0'][0].weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['0'][0].covariances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(models['0'][0].weights_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempint = 0\n",
    "get_size(tempint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect.getmembers(models['0'][0].means_, lambda a:not(inspect.isroutine(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = inspect.getmembers(models['0'][0], lambda a:not(inspect.isroutine(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a for a in attributes if not(a[0].startswith('__') and a[0].endswith('__'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybool = True==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssTableData[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIndi2(ssTableData[0][:100],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
